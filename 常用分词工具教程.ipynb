{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\JIANGH~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.698 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['化妆', '和', '服装']\n"
     ]
    }
   ],
   "source": [
    "jieba.initialize()\n",
    "text = '化妆和服装'\n",
    "words = jieba.cut(text)\n",
    "words = list(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HanLP汉语言处理包\n",
    "HanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhanlp\n",
    "text = '化妆和服装'\n",
    "words = []\n",
    "for term in pyhanlp.HanLP.segment(text):\n",
    "\twords.append(term.word)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "snowNLP中文的类库\n",
    "SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snownlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['化妆', '和', '服装']\n"
     ]
    }
   ],
   "source": [
    "text = '化妆和服装'\n",
    "words = snownlp.SnowNLP(text).words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FoolNLTK中文处理工具包\n",
    "可能不是最快的开源中文分词，但很可能是最准的开源中文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fool\n",
    "\n",
    "text = '化妆和服装'\n",
    "\n",
    "words = fool.cut(text)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jiagu甲骨NLP\n",
    "基于BiLSTM模型，使用大规模语料训练而成。将提供中文分词、词性标注、命名实体识别、关键词抽取、文本摘要、新词发现等常用自然语言处理功能。参考了各大工具优缺点制作，将Jiagu回馈给大家。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiagu\n",
    "\n",
    "jiagu.init()\n",
    "\n",
    "text = '化妆和服装'\n",
    "\n",
    "words = jiagu.seg(text)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyltp（哈工大语言云）\n",
    "pyltp 是 LTP 的 Python 封装，提供了分词，词性标注，命名实体识别，依存句法分析，语义角色标注的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltp\n",
    "\n",
    "segmentor = pyltp.Segmentor()\n",
    "segmentor.load('model/ltp_data_v3.4.0/cws.model') # 模型放置的路径\n",
    "\n",
    "text = '化妆和服装'\n",
    "\n",
    "words = segmentor.segment(text)\n",
    "\n",
    "words = list(words)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THULAC（清华中文词法分析工具包）\n",
    "THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thulac\n",
    "\n",
    "thu = thulac.thulac(seg_only=True)\n",
    "\n",
    "text = '化妆和服装'\n",
    "\n",
    "words = thu.cut(text, text=True).split()\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLPIR（汉语分词系统）主要功能包括中文分词；英文分词；词性标注；命名实体识别；新词识别；关键词提取；支持用户专业词典与微博分析。NLPIR系统支持多种编码、多种操作系统、多种开发语言与平台。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynlpir\n",
    "\n",
    "pynlpir.open()\n",
    "\n",
    "text = '化妆和服装'\n",
    "\n",
    "words = pynlpir.segment(text, pos_tagging=False)\n",
    "\n",
    "print(words)\n",
    "\n",
    "pynlpir.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
