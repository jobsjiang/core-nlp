{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然语言处理中有两种文本摘要生成方法：抽取式和抽象式，尽管抽象式摘要的表现更好，但开发相关算法需要复杂但深度学习技巧和语言模型，为了获得合理产出，抽象式摘要方法必须能够解决诸多自然语言处理问题，如自然语言生成，语义表征和推理排序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 第一步：将这段话转换成句子\n",
    "* 第二步：文本处理，移除停止词，数字，标点符号以及句子中的其他特殊字符。句子成分的过滤有助于移除冗余和不重要的信息。\n",
    "* 第三步：分词\n",
    "* 第四步：评估单词的加权出现频率（用每个单词的出现频率除以这段话中出现最多次的单词的频率）\n",
    "* 第五步：用相应的加权频率替代原句中的各个单词，然后计算总和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T15:31:21.067665Z",
     "start_time": "2019-07-17T15:30:04.748096Z"
    }
   },
   "outputs": [],
   "source": [
    "# 第一步：准备数据\n",
    "import bs4 as BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "# fetched_data = urllib3.request.urlopen('https://en.wikipedia.org/wiki/20th_century')\n",
    "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/20th_century')\n",
    "article_read = fetched_data.read()\n",
    "article_parse = BeautifulSoup.BeautifulSoup(article_read,'lxml')\n",
    "\n",
    "paragraphs = article_parse.find_all('p')\n",
    "article_content = ''\n",
    "for p in paragraphs:\n",
    "    article_content += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二步：处理数据\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # 将单词还原成词根形式的算法\n",
    "from nltk.tokenize import  word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_dictionary_table(text_string) -> dict:  # 描述函数的返回类型\n",
    "    # 移除停止词\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text_string)  # 对句子进行分词\n",
    "    # 还原词根\n",
    "    stem = PorterStemmer()\n",
    "    # 为词频表创建词典\n",
    "    frequency_table = dict()  # 创建词典\n",
    "    for wd in words:\n",
    "        wd = stem.stem(wd)\n",
    "        if wd in stop_words:\n",
    "            continue\n",
    "        if wd in frequency_table:\n",
    "            frequency_table[wd] += 1\n",
    "        else:\n",
    "            frequency_table[wd] = 1\n",
    "    return frequency_table\n",
    "\n",
    "\n",
    "\n",
    "# def _create_dictionary_table(text_string) -> dict:\n",
    "\n",
    "#     # removing stop words\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#     words = word_tokenize(text_string)\n",
    "\n",
    "#     # reducing words to their root form\n",
    "#     stem = PorterStemmer()\n",
    "\n",
    "#     # creating dictionary for the word frequency table\n",
    "#     frequency_table = dict()\n",
    "#     for wd in words:\n",
    "#         wd = stem.stem(wd)\n",
    "#         if wd in stop_words:\n",
    "#             continue\n",
    "#         if wd in frequency_table:\n",
    "#             frequency_table[wd] += 1\n",
    "#         else:\n",
    "#             frequency_table[wd] = 1\n",
    "\n",
    "#     return frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frequency_table = _create_dictionary_table(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三步：将文章分割成句子\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "sentences = sent_tokenize(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第四步：确定句子的加权频率\n",
    "def _calculate_sentence_scores(sentences,frequency_tabel) -> dict:\n",
    "    sentence_weight = dict()\n",
    "    for sentence in sentences:\n",
    "        sentence_wordcount = (len(word_tokenize(sentence)))\n",
    "        sentence_wordcount_without_stop_words = 0\n",
    "        for word_weight in frequency_tabel:\n",
    "            if word_weight in sentence.lower():\n",
    "                sentence_wordcount_without_stop_words += 1\n",
    "                if sentence[:7] in sentence_weight:\n",
    "                    sentence_weight[sentence[:7]] += frequency_tabel[word_weight]\n",
    "                else:\n",
    "                    sentence_weight[sentence[:7]] = frequency_tabel[word_weight]\n",
    "        # 为了避免长句的分数必然高于短句，我们用每个句子的分数除以该句中的单词数\n",
    "        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] / sentence_wordcount_without_stop_words\n",
    "    return sentence_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_weight = _calculate_sentence_scores(sentences,frequency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第五步：计算句子阈值\n",
    "def _calculate_average_score(sentence_weight) -> int:\n",
    "    sum_values = 0\n",
    "    for entry in sentence_weight:\n",
    "        sum_values += sentence_weight[entry]\n",
    "    average_score = (sum_values) / len(sentence_weight)\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第六步：生成摘要\n",
    "def _get_article_summary(sentences,sentence_weight,threshold):\n",
    "    sentence_counter = 0\n",
    "    article_summary = ''\n",
    "    for sentence in sentences:\n",
    "        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n",
    "            article_summary += \" \" + sentence\n",
    "            sentence_counter += 1\n",
    "    return article_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import bs4 as BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "# fetching the content from the URL\n",
    "fetched_data = urllib.request.urlopen(\n",
    "    'https://en.wikipedia.org/wiki/20th_century')\n",
    "\n",
    "article_read = fetched_data.read()\n",
    "\n",
    "# parsing the URL content and storing in a variable\n",
    "article_parsed = BeautifulSoup.BeautifulSoup(article_read, 'html.parser')\n",
    "\n",
    "#returning <p> tags\n",
    "paragraphs = article_parsed.find_all('p')\n",
    "\n",
    "article_content = ''\n",
    "\n",
    "# looping through the paragraphs and adding them to the variable\n",
    "for p in paragraphs:\n",
    "    article_content += p.text\n",
    "\n",
    "\n",
    "def _create_dictionary_table(text_string) -> dict:\n",
    "\n",
    "    # removing stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    words = word_tokenize(text_string)\n",
    "\n",
    "    # reducing words to their root form\n",
    "    stem = PorterStemmer()\n",
    "\n",
    "    # creating dictionary for the word frequency table\n",
    "    frequency_table = dict()\n",
    "    for wd in words:\n",
    "        wd = stem.stem(wd)\n",
    "        if wd in stop_words:\n",
    "            continue\n",
    "        if wd in frequency_table:\n",
    "            frequency_table[wd] += 1\n",
    "        else:\n",
    "            frequency_table[wd] = 1\n",
    "\n",
    "    return frequency_table\n",
    "\n",
    "\n",
    "def _calculate_sentence_scores(sentences, frequency_table) -> dict:\n",
    "\n",
    "    # algorithm for scoring a sentence by its words\n",
    "    sentence_weight = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_wordcount = (len(word_tokenize(sentence)))\n",
    "        sentence_wordcount_without_stop_words = 0\n",
    "        for word_weight in frequency_table:\n",
    "            if word_weight in sentence.lower():\n",
    "                sentence_wordcount_without_stop_words += 1\n",
    "                if sentence[:7] in sentence_weight:\n",
    "                    sentence_weight[sentence[:7]\n",
    "                                    ] += frequency_table[word_weight]\n",
    "                else:\n",
    "                    sentence_weight[sentence[:7]\n",
    "                                    ] = frequency_table[word_weight]\n",
    "\n",
    "        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]\n",
    "                                                        ] / sentence_wordcount_without_stop_words\n",
    "\n",
    "    return sentence_weight\n",
    "\n",
    "\n",
    "def _calculate_average_score(sentence_weight) -> int:\n",
    "\n",
    "    # calculating the average score for the sentences\n",
    "    sum_values = 0\n",
    "    for entry in sentence_weight:\n",
    "        sum_values += sentence_weight[entry]\n",
    "\n",
    "    # getting sentence average value from source text\n",
    "    average_score = (sum_values / len(sentence_weight))\n",
    "\n",
    "    return average_score\n",
    "\n",
    "\n",
    "def _get_article_summary(sentences, sentence_weight, threshold):\n",
    "    sentence_counter = 0\n",
    "    article_summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n",
    "            article_summary += \" \" + sentence\n",
    "            sentence_counter += 1\n",
    "\n",
    "    return article_summary\n",
    "\n",
    "\n",
    "def _run_article_summary(article):\n",
    "\n",
    "    # creating a dictionary for the word frequency table\n",
    "    frequency_table = _create_dictionary_table(article)\n",
    "\n",
    "    # tokenizing the sentences\n",
    "    sentences = sent_tokenize(article)\n",
    "\n",
    "    # algorithm for scoring a sentence by its words\n",
    "    sentence_scores = _calculate_sentence_scores(sentences, frequency_table)\n",
    "\n",
    "    # getting the threshold\n",
    "    threshold = _calculate_average_score(sentence_scores)\n",
    "\n",
    "    # producing the summary\n",
    "    article_summary = _get_article_summary(\n",
    "        sentences, sentence_scores, 1.5 * threshold)\n",
    "\n",
    "    return article_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results = _run_article_summary(article_content)\n",
    "print(summary_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
