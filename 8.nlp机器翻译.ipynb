{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/xiaosongshine/article/details/99843848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import opencc as oc\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read2df(mnt_txt):\n",
    "    cc = oc.OpenCC('t2s')\n",
    "    with open(mnt_txt,'r',encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    data_list = data.split('\\n')\n",
    "    eng_list,chn_list = [],[]\n",
    "    df = pd.DataFrame()\n",
    "    for dl in data_list[:-1]:\n",
    "        dls = dl.split('\\t')\n",
    "#         print(dls)\n",
    "        eng_list.append(split_dot(dls[0]))\n",
    "        chn_list.append(cc.convert(dls[1]))\n",
    "    df['eng'] = eng_list\n",
    "    df['chn'] = chn_list\n",
    "    print(df.head(5))\n",
    "    df.to_csv('../dataset/cmn.csv',index=None)\n",
    "    print('save csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dot(strs,dots=\", . ! ?\"):\n",
    "    for d in dots.split(' '):\n",
    "        strs = strs.replace(d,' '+d)\n",
    "    return strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eng_dicts(datas):\n",
    "    w_all_dict = {}\n",
    "    for sample in datas:\n",
    "        for token in sample.split(' '):\n",
    "            if token not in w_all_dict.keys():\n",
    "                w_all_dict[token] = 1\n",
    "            else:\n",
    "                w_all_dict[token] += 1\n",
    "    sort_w_list = sorted(w_all_dict.items(),key=lambda d:d[1],reverse=True)\n",
    "#     print(sort_w_list)\n",
    "    w_keys = [x for x,_ in sort_w_list[:7000-2]]\n",
    "#     print(w_keys)\n",
    "#     print(type(w_keys))\n",
    "    w_keys.insert(0,'<PAD>')\n",
    "    w_keys.insert(0,'<UNK>')\n",
    "    \n",
    "    w_dict = {x:i for i,x in enumerate(w_keys)}\n",
    "    i_dict = {i:x for i,x in enumerate(w_keys)}\n",
    "    return w_dict,i_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chn_dicts(datas):\n",
    "    w_all_dict = {}\n",
    "    for sample in datas:\n",
    "        for token in jieba.cut(sample):\n",
    "            if token not in w_all_dict.keys():\n",
    "                w_all_dict[token] = 1\n",
    "            else:\n",
    "                w_all_dict[token] += 1\n",
    " \n",
    "    sort_w_list = sorted(w_all_dict.items(),  key=lambda d: d[1], reverse=True)\n",
    "\n",
    "    w_keys = [x for x,_ in sort_w_list[:10000-4]]\n",
    "    w_keys.insert(0,\"<EOS>\")\n",
    "    w_keys.insert(0,\"<GO>\")\n",
    "    w_keys.insert(0,\"<PAD>\")\n",
    "    w_keys.insert(0,\"<UNK>\")\n",
    "    w_dict = { x:i for i,x in enumerate(w_keys) }\n",
    "    i_dict = { i:x for i,x in enumerate(w_keys) }\n",
    "    return w_dict,i_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(keys,dicts):\n",
    "    if keys in dicts.keys():\n",
    "        val = dicts[keys]\n",
    "    else:\n",
    "        keys = '<UNK>'\n",
    "        val = dicts[keys]\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(lists,lens=LENS):\n",
    "    list_ret = []\n",
    "    for l in lists:\n",
    "        while(len(l) < lens):\n",
    "            l.append(1)\n",
    "        if len(l) > lens:\n",
    "            l = l[:lens]\n",
    "        list_ret.append(l)\n",
    "    return list_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       eng    chn\n",
      "0     Hi .     嗨。\n",
      "1     Hi .    你好。\n",
      "2    Run .  你用跑的。\n",
      "3   Wait !    等等！\n",
      "4  Hello !    你好。\n",
      "save csv\n"
     ]
    }
   ],
   "source": [
    "df = read2df('../dataset/cmn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\jieba.cache\n",
      "Loading model cost 0.538 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<UNK>', '<PAD>', '.', 'I', 'to', 'the', 'a', 'you', '?', 'is', 'Tom', 'He', 'in', 'of', 'me', ',', 'was', 'for', 'have', 'The']\n",
      "['<UNK>', '<PAD>', '<GO>', '<EOS>', '。', '我', '的', '了', '你', '他', '？', '在', '汤姆', '是', '她', '吗', '我们', '，', '不', '很']\n"
     ]
    }
   ],
   "source": [
    "eng_dict,id2eng = get_eng_dicts(df['eng'])\n",
    "chn_dict,id2chn = get_chn_dicts(df['chn'])\n",
    "print(list(eng_dict.keys())[:20])\n",
    "print(list(chn_dict.keys())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预定义模型参数\n",
    "EN_VOCAB_SIZE = 7000\n",
    "CH_VOCAB_SIZE = 10000\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,LSTM,Dense,Embedding,CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # encoder\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    emb_inp = Embedding(output_dim=128,input_dim=EN_VOCAB_SIZE)(encoder_inputs)\n",
    "    encoder_h1,encoder_state_h1,encoder_state_c1 = CuDNNLSTM(HIDDEN_SIZE,return_sequences=True,return_state=True)(emb_inp)\n",
    "    encoder_h2,encoder_state_h2,encoder_state_c2 = CuDNNLSTM(HIDDEN_SIZE,return_state=True)(encoder_h1)\n",
    "    # decoder\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    emb_target = Embedding(output_dim=128,input_dim=CH_VOCAB_SIZE)(decoder_inputs)\n",
    "    lstm1 = CuDNNLSTM(HIDDEN_SIZE,return_sequences=True,return_state=True)\n",
    "    lstm2 = CuDNNLSTM(HIDDEN_SIZE,return_sequences=True,return_state=True)\n",
    "    decoder_dense = Dense(CH_VOCAB_SIZE,activation='softmax')\n",
    "    \n",
    "    decoder_h1, _, _ = lstm1(emb_target, initial_state=[encoder_state_h1, encoder_state_c1])\n",
    "    decoder_h2, _, _ = lstm2(decoder_h1, initial_state=[encoder_state_h2, encoder_state_c2])\n",
    "    decoder_outputs = decoder_dense(decoder_h2)\n",
    "    \n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # encoder模型和训练相同\n",
    "    encoder_model = Model(encoder_inputs, [encoder_state_h1, encoder_state_c1, encoder_state_h2, encoder_state_c2])\n",
    "\n",
    "    # 预测模型中的decoder的初始化状态需要传入新的状态\n",
    "    decoder_state_input_h1 = Input(shape=(HIDDEN_SIZE,))\n",
    "    decoder_state_input_c1 = Input(shape=(HIDDEN_SIZE,))\n",
    "    decoder_state_input_h2 = Input(shape=(HIDDEN_SIZE,))\n",
    "    decoder_state_input_c2 = Input(shape=(HIDDEN_SIZE,))\n",
    "\n",
    "    # 使用传入的值来初始化当前模型的输入状态\n",
    "    decoder_h1, state_h1, state_c1 = lstm1(emb_target, initial_state=[decoder_state_input_h1, decoder_state_input_c1])\n",
    "    decoder_h2, state_h2, state_c2 = lstm2(decoder_h1, initial_state=[decoder_state_input_h2, decoder_state_input_c2])\n",
    "    decoder_outputs = decoder_dense(decoder_h2)\n",
    "\n",
    "    decoder_model = Model([decoder_inputs, decoder_state_input_h1, decoder_state_input_c1, decoder_state_input_h2, decoder_state_input_c2], \n",
    "                        [decoder_outputs, state_h1, state_c1, state_h2, state_c2])\n",
    "    \n",
    "    return model,encoder_model,decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_acc(y_true,y_pred):\n",
    "    acc = K.cast(K.equal(K.max(y_true,axis=-1),K.cast(K.argmax(y_pred,axis=-1),K.floatx())),K.floatx())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_in = [[get_val(e,eng_dict) for e in eng.split(' ')] for eng in df['eng']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_in = [[get_val(\"<GO>\",chn_dict)]+[get_val(e,chn_dict) for e in jieba.cut(eng)]+[get_val(\"<EOS>\",chn_dict)] for eng in df[\"chn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    " dec_out = [[get_val(e,chn_dict) for e in jieba.cut(eng)]+[get_val(\"<EOS>\",chn_dict)] for eng in df[\"chn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_in_ar = np.array(padding(enc_in, 32))\n",
    "dec_in_ar = np.array(padding(dec_in, 30))\n",
    "dec_out_ar = np.array(padding(dec_out, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model = get_model()\n",
    "\n",
    "model.load_weights('e2c1.h5')\n",
    "\n",
    "opt = Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.99, epsilon=1e-08)\n",
    "model.compile(\n",
    "    optimizer=opt, loss='sparse_categorical_crossentropy', metrics=[my_acc])\n",
    "model.summary()\n",
    "print(dec_out_ar.shape)\n",
    "model.fit([enc_in_ar, dec_in_ar], np.expand_dims(dec_out_ar, -1),\n",
    "          batch_size=50,\n",
    "          epochs=64,\n",
    "          initial_epoch=56,\n",
    "          validation_split=0.1)\n",
    "model.save('e2c1.h5')\n",
    "encoder_model.save(\"enc1.h5\")\n",
    "decoder_model.save(\"dec1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "encoder_model, decoder_model = load_model(\"enc1.h5\", custom_objects={\n",
    "                                          \"my_acc\": my_acc}), load_model(\"dec1.h5\", custom_objects={\"my_acc\": my_acc})\n",
    "\n",
    "for k in range(16000-20, 16000):\n",
    "        test_data = enc_in_ar[k:k+1]\n",
    "        h1, c1, h2, c2 = encoder_model.predict(test_data)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "\n",
    "        outputs = []\n",
    "        target_seq[0, len(outputs)] = chn_dict[\"<GO>\"]\n",
    "        while True:\n",
    "            output_tokens, h1, c1, h2, c2 = decoder_model.predict(\n",
    "                [target_seq, h1, c1, h2, c2])\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            # print(sampled_token_index)\n",
    "            outputs.append(sampled_token_index)\n",
    "            #target_seq = np.zeros((1, 30))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "            # print(target_seq)\n",
    "            if sampled_token_index == chn_dict[\"<EOS>\"] or len(outputs) > 28:\n",
    "                break\n",
    "\n",
    "        print(\"> \"+df[\"eng\"][k])\n",
    "        print(\"< \"+' '.join([id2chn[i] for i in outputs[:-1]]))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
