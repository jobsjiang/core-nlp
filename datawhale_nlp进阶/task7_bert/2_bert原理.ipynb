{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT的预训练过程\n",
    "接下来我们看看BERT的预训练过程，BERT的预训练阶段采用了两个独有的非监督任务，一个是Masked Language Model，还有一个是Next Sentence Prediction。\n",
    "#### Masked Language Model\n",
    "mlm可以理解为完形填空，作者会随机mask每一个句子中15%的词，用其上下文来做预测，例如：my dog is hairy → my dog is [MASK]\n",
    "\n",
    "此处将hairy进行了mask处理，然后采用非监督学习的方法预测mask位置的词是什么，但是该方法有一个问题，因为是mask15%的词，其数量已经很高了，这样就会导致某些词在fine-tuning阶段从未见过，为了解决这个问题，作者做了如下的处理：\n",
    "\n",
    "* 80%的时间是采用[mask]，my dog is hairy → my dog is [MASK]\n",
    "* 10%的时间是随机取一个词来代替mask的词，my dog is hairy -> my dog is apple\n",
    "* 10%的时间保持不变，my dog is hairy -> my dog is hairy\n",
    "那么为啥要以一定的概率使用随机词呢？这是因为transformer要保持对每个输入token分布式的表征，否则Transformer很可能会记住这个[MASK]就是\"hairy\"。至于使用随机词带来的负面影响，文章中说了,所有其他的token(即非\"hairy\"的token)共享15%*10% = 1.5%的概率，其影响是可以忽略不计的。\n",
    "#### Next Sentence Prediction\n",
    "选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。\n",
    "#### 输入\n",
    "bert的输入可以是单一的一个句子或者是句子对，实际的输入值包括了三个部分，分别是token embedding词向量，segment embedding句向量，每个句子有个句子整体的embedding项对应给每个单词，还有position embedding位置向量，这三个部分相加形成了最终的bert输入向量。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
