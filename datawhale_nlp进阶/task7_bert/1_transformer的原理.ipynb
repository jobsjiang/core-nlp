{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](https://img-blog.csdnimg.cn/20190530153658185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p5cTExMjIz,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器\n",
    "由6个相同的层堆叠在一起，每一层又有两个支层。第一个支层是一个多头的自注意机制(Query = Key = Value)，第二个支层是一个全连接前馈网络。在两个子层中会使用一个残差连接，接着进行层标准化(layer normalization)。也就是说每一个子层的输出都是LayerNorm(x + sublayer(x))。为了方便进行残差连接，我们需要子层的输出和输入都是相同的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器\n",
    "也是堆叠了6个相同的层，每一层有三个支层。第一个支层是一个多头的自注意机制(Query = Key = Value)；第二个支层是一个多头注意机制(Key = Value = Encoder Self Attention, Query = Decoder Self Attention)，这层的输入：一方面来自第一个之层的输出，另一方面是编码器中第i (i=1,2,3,4,5,6)层的输出，作为对于解码器中第i (i=1,2,3,4,5,6)层的输入；第三个支层是一个全连接前馈网络；经过Add&Norm操作后，经过一个线性+softmax变换得到最后目标输出的概率。对于解码器中的第一个多头注意力层，需要添加masking，确保预测位置i仅仅依赖于位置小于i的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
