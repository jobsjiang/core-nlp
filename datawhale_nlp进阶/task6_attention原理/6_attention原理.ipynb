{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核心思想\n",
    "Attention的思想理解起来比较容易，就是在decoding阶段对input的信息赋予不同权重，在nlp中就是针对sequence的每个time step input,在cv中就是针对每个pixpl\n",
    "#### 模型分类\n",
    "1. Soft/Hard Attention\n",
    "\n",
    "soft attention：传统attention，可被嵌入到模型中去进行训练并传播梯度\n",
    "\n",
    "hard attention：不计算所有输出，依据概率对encoder的输出采样，在反向传播时需采用蒙特卡洛进行梯度估计\n",
    "\n",
    "2.  Global/Local Attention\n",
    "\n",
    "global attention：传统attention，对所有encoder输出进行计算\n",
    "\n",
    "local attention：介于soft和hard之间，会预测一个位置并选取一个窗口进行计算\n",
    "\n",
    "3. Self Attention\n",
    "\n",
    "传统attention是计算Q和K之间的依赖关系，而self attention则分别计算Q和K自身的依赖关系。具体的详解会在下篇文章给出~\n",
    "#### 优缺点\n",
    "优点：\n",
    "\n",
    "在输出序列与输入序列“顺序”不同的情况下表现较好，如翻译、阅读理解\n",
    "相比RNN可以编码更长的序列信息\n",
    "缺点：\n",
    "\n",
    "对序列顺序不敏感\n",
    "通常和RNN结合使用，不能并行化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
