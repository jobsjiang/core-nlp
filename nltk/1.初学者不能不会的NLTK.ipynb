{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'natural', 'language', 'processing', '!']\n"
     ]
    }
   ],
   "source": [
    "# 1.tokenize\n",
    "import nltk\n",
    "sentence = 'I love natural language processing!'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('love', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 2.词性标注\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading ../../dataset/maxent_ne_chunker: Package\n",
      "[nltk_data]     '../../dataset/maxent_ne_chunker' not found in index\n",
      "[nltk_data] Error loading ../../dataset/words: Package\n",
      "[nltk_data]     '../../dataset/words' not found in index\n",
      "(S I/PRP love/VBP natural/JJ language/NN processing/NN !/.)\n"
     ]
    }
   ],
   "source": [
    "# 命名实体识别\n",
    "nltk.download('../../dataset/maxent_ne_chunker') # 下载模型\n",
    "nltk.download('../../dataset/words')\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.下载语料库 系统已经下载brown语料库\n",
    "# nltk.download('../../dataset/brown') #例如：下载brown，更多语料库：http://www.nltk.org/howto/corpus.html\n",
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:1.0\n",
      "recall:0.8\n",
      "f_measure:0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# 5. 度量\n",
    "# precision:正确率，recall:召回率，f_measure\n",
    "from nltk.metrics import precision,recall,f_measure\n",
    "reference = 'DET NN VB DET JJ NN NN IN DET NN'.split()\n",
    "test = 'DET VB VB DET NN NN NN IN DET NN'.split()\n",
    "reference_set = set(reference)\n",
    "test_set = set(test)\n",
    "print(\"precision:\" + str(precision(reference_set, test_set)))\n",
    "print(\"recall:\" + str(recall(reference_set, test_set)))\n",
    "print(\"f_measure:\" + str(f_measure(reference_set,test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni\n",
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# 6.词干提取（Stemmers）\n",
    "from nltk.stem.porter import * # Porter stemmer\n",
    "# 创建词干提取器\n",
    "stemmer = PorterStemmer()\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer # Snowball stemmer\n",
    "print(' '.join(SnowballStemmer.languages))\n",
    "# 指定语言\n",
    "stemmer = SnowballStemmer('english')\n",
    "print(stemmer.stem('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<breakdown.n.03: PosScore=0.0 NegScore=0.25>\n",
      "0.0\n",
      "0.25\n",
      "0.75\n",
      "[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'), SentiSynset('slow.v.03'), SentiSynset('slow.a.01'), SentiSynset('slow.a.02'), SentiSynset('dense.s.04'), SentiSynset('slow.a.04'), SentiSynset('boring.s.01'), SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'), SentiSynset('behind.r.03')]\n",
      "[SentiSynset('happy.a.01'), SentiSynset('felicitous.s.02'), SentiSynset('glad.s.02'), SentiSynset('happy.s.04')]\n"
     ]
    }
   ],
   "source": [
    "# 7.SentiWordNet接口 系统已经下载\n",
    "# nltk.download('../../dataset/sentiwordnet') # 下载sentiwordnet词典\n",
    "# SentiSynsets:synsets(同义词集)的情感值\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "breakdown = swn.senti_synset('breakdown.n.03')\n",
    "print(breakdown)\n",
    "print(breakdown.pos_score())\n",
    "print(breakdown.neg_score())\n",
    "print(breakdown.obj_score())\n",
    "# 查看\n",
    "print(list(swn.senti_synsets('slow')))\n",
    "happy = swn.senti_synsets('happy','a')\n",
    "print(list(happy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
