{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = codecs.open(\"../dataset/socialmedia_relevant_cols.csv\", \"r\",encoding='utf-8', errors='replace')\n",
    "output_file = open(\"../dataset/socialmedia_relevant_cols_clean.csv\", \"w\",encoding='utf-8')\n",
    "\n",
    "def sanitize_characters(raw, clean):    \n",
    "    for line in input_file:\n",
    "        out = line\n",
    "        output_file.write(line)\n",
    "sanitize_characters(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions = pd.read_csv(\"../dataset/socialmedia_relevant_cols_clean.csv\")\n",
    "questions.columns=['text', 'choose_one', 'class_label']\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "questions = standardize_text(questions,\"text\")\n",
    "questions.to_csv('../dataset/clean_data.csv')\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = pd.read_csv(\"../dataset/clean_data.csv\")\n",
    "clean_questions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions.groupby('class_label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "clean_questions['tokens'] = clean_questions['text'].apply(tokenizer.tokenize)\n",
    "clean_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for tokens in clean_questions['tokens'] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in clean_questions['tokens']]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" %(len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.hist(sentence_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb,count_vectorizer\n",
    "list_corpus = clean_questions['text'].tolist()\n",
    "list_labels = clean_questions['class_label'].tolist()\n",
    "X_train,X_test,y_train,y_test = train_test_split(list_corpus,list_labels,test_size=0.2,random_state=40)\n",
    "X_train_counts,count_vectorizer = cv(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n",
    "    lsa = TruncatedSVD(n_components=2)\n",
    "    lsa.fit(test_data)\n",
    "    lsa_scores = lsa.transform(test_data)\n",
    "    color_mapper = {label: idx for idx, label in enumerate(set(test_labels))}\n",
    "    color_column = [color_mapper[label] for label in test_labels]\n",
    "    colors = ['orange', 'blue', 'blue']\n",
    "    if plot:\n",
    "        plt.scatter(lsa_scores[:, 0], lsa_scores[:, 1], s=8, alpha=.8,\n",
    "                    c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "        red_patch = mpatches.Patch(color='orange', label='Irrelevant')\n",
    "        green_patch = mpatches.Patch(color='blue', label='Disaster')\n",
    "        plt.legend(handles=[red_patch, green_patch], prop={'size': 30})\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "plot_LSA(X_train_counts, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=30.0,class_weight='balanced',solver='newton-cg',multi_class='multinomial',n_jobs=1,random_state=40)\n",
    "clf.fit(X_train_counts,y_train)\n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_test,y_predicted):\n",
    "    precision = precision_score(y_test,y_predicted,pos_label=None,average='weighted')\n",
    "    recall = recall_score(y_test,y_predicted,pos_label=None,average='weighted')\n",
    "    f1 = f1_score(y_test,y_predicted,pos_label=None,average='weighted')\n",
    "    accuracy = accuracy_score(y_test,y_predicted)\n",
    "    return precision,recall,f1,accuracy\n",
    "precision,recall,f1,accuracy = get_metrics(y_test,y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predicted_counts)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=[\n",
    "                             'Irrelevant', 'Disaster', 'Unsure'], normalize=False, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "    # loop for each class\n",
    "    classes = {}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i])\n",
    "                            for i, el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(\n",
    "            word_importances, key=lambda x: x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key=lambda x: x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops': tops,\n",
    "            'bottom': bottom\n",
    "        }\n",
    "    return classes\n",
    "importance = get_most_important_features(count_vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a, b) for a, b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "\n",
    "    bottom_pairs = [(a, b) for a, b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "\n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos, bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Irrelevant', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos, top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Disaster', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "top_scores = [a[0] for a in importance[1]['tops']]\n",
    "top_words = [a[1] for a in importance[1]['tops']]\n",
    "bottom_scores = [a[0] for a in importance[1]['bottom']]\n",
    "bottom_words = [a[1] for a in importance[1]['bottom']]\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores,\n",
    "                     bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(X_train_tfidf, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf = get_metrics(y_test, y_predicted_tfidf)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_tfidf, precision_tfidf, \n",
    "                                                                       recall_tfidf, f1_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(y_test, y_predicted_tfidf)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm2, classes=['Irrelevant','Disaster','Unsure'], normalize=False, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(\"TFIDF confusion matrix\")\n",
    "print(cm2)\n",
    "print(\"BoW confusion matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at important coefficients for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_tfidf = get_most_important_features(tfidf_vectorizer, clf_tfidf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores = [a[0] for a in importance_tfidf[1]['tops']]\n",
    "top_words = [a[1] for a in importance_tfidf[1]['tops']]\n",
    "bottom_scores = [a[0] for a in importance_tfidf[1]['bottom']]\n",
    "bottom_words = [a[1] for a in importance_tfidf[1]['bottom']]\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing semantic meaning\n",
    "### Enter word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = '../dataset/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(\n",
    "            k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(\n",
    "            k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
    "    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors,\n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_word2vec_embeddings(word2vec, clean_questions)\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels, \n",
    "                                                                                        test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(embeddings, list_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', random_state=40)\n",
    "clf_w2v.fit(X_train_word2vec, y_train_word2vec)\n",
    "y_predicted_word2vec = clf_w2v.predict(X_test_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n",
    "                                                                       recall_word2vec, f1_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_w2v = confusion_matrix(y_test_word2vec, y_predicted_word2vec)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['Irrelevant','Disaster','Unsure'], normalize=False, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(\"Word2Vec confusion matrix\")\n",
    "print(cm_w2v)\n",
    "print(\"TFIDF confusion matrix\")\n",
    "print(cm2)\n",
    "print(\"BoW confusion matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(list_corpus, list_labels, test_size=0.2, \n",
    "                                                                                random_state=40)\n",
    "vector_store = word2vec\n",
    "def word2vec_pipeline(examples):\n",
    "    global vector_store\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_list = []\n",
    "    for example in examples:\n",
    "        example_tokens = tokenizer.tokenize(example)\n",
    "        vectorized_example = get_average_word2vec(example_tokens, vector_store, generate_missing=False, k=300)\n",
    "        tokenized_list.append(vectorized_example)\n",
    "    return clf_w2v.predict_proba(tokenized_list)\n",
    "\n",
    "c = make_pipeline(count_vectorizer, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_one_instance(instance, class_names):\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    exp = explainer.explain_instance(instance, word2vec_pipeline, num_features=6)\n",
    "    return exp\n",
    "\n",
    "def visualize_one_exp(features, labels, index, class_names = [\"irrelevant\",\"relevant\", \"unknown\"]):\n",
    "    exp = explain_one_instance(features[index], class_names = class_names)\n",
    "    print('Index: %d' % index)\n",
    "    print('True class: %s' % class_names[labels[index]])\n",
    "    exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_one_exp(X_test_data, y_test_data, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_one_exp(X_test_data, y_test_data, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "random.seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistical_explanation(test_set, sample_size, word2vec_pipeline, label_dict):\n",
    "    sample_sentences = random.sample(test_set, sample_size)\n",
    "    explainer = LimeTextExplainer()\n",
    "    \n",
    "    labels_to_sentences = defaultdict(list)\n",
    "    contributors = defaultdict(dict)\n",
    "    \n",
    "    # First, find contributing words to each class\n",
    "    for sentence in sample_sentences:\n",
    "        probabilities = word2vec_pipeline([sentence])\n",
    "        curr_label = probabilities[0].argmax()\n",
    "        labels_to_sentences[curr_label].append(sentence)\n",
    "        exp = explainer.explain_instance(sentence, word2vec_pipeline, num_features=6, labels=[curr_label])\n",
    "        listed_explanation = exp.as_list(label=curr_label)\n",
    "        \n",
    "        for word,contributing_weight in listed_explanation:\n",
    "            if word in contributors[curr_label]:\n",
    "                contributors[curr_label][word].append(contributing_weight)\n",
    "            else:\n",
    "                contributors[curr_label][word] = [contributing_weight]    \n",
    "    \n",
    "    # average each word's contribution to a class, and sort them by impact\n",
    "    average_contributions = {}\n",
    "    sorted_contributions = {}\n",
    "    for label,lexica in contributors.items():\n",
    "        curr_label = label\n",
    "        curr_lexica = lexica\n",
    "        average_contributions[curr_label] = pd.Series(index=curr_lexica.keys())\n",
    "        for word,scores in curr_lexica.items():\n",
    "            average_contributions[curr_label].loc[word] = np.sum(np.array(scores))/sample_size\n",
    "        detractors = average_contributions[curr_label].sort_values()\n",
    "        supporters = average_contributions[curr_label].sort_values(ascending=False)\n",
    "        sorted_contributions[label_dict[curr_label]] = {\n",
    "            'detractors':detractors,\n",
    "             'supporters': supporters\n",
    "        }\n",
    "    return sorted_contributions\n",
    "\n",
    "label_to_text = {\n",
    "    0: 'Irrelevant',\n",
    "    1: 'Relevant',\n",
    "    2: 'Unsure'\n",
    "}\n",
    "sorted_contributions = get_statistical_explanation(X_test_data, 100, word2vec_pipeline, label_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First index is the class (Disaster)\n",
    "# Second index is 0 for detractors, 1 for supporters\n",
    "# Third is how many words we sample\n",
    "top_words = sorted_contributions['Relevant']['supporters'][:10].index.tolist()\n",
    "top_scores = sorted_contributions['Relevant']['supporters'][:10].tolist()\n",
    "bottom_words = sorted_contributions['Relevant']['detractors'][:10].index.tolist()\n",
    "bottom_scores = sorted_contributions['Relevant']['detractors'][:10].tolist()\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging text structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNNs for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "VALIDATION_SPLIT=.2\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(clean_questions[\"text\"].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(clean_questions[\"text\"].tolist())\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "cnn_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(clean_questions[\"class_label\"]))\n",
    "\n",
    "indices = np.arange(cnn_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "cnn_data = cnn_data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * cnn_data.shape[0])\n",
    "\n",
    "embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in word_index.items():\n",
    "    embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "#     l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "\n",
    "    preds = Dense(labels_index, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = cnn_data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = cnn_data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "model = ConvNet(embedding_weights, MAX_SEQUENCE_LENGTH, len(word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(clean_questions[\"class_label\"].unique())), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about text generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/ajmanser/Yelp\n",
    "\n",
    "from keras import layers\n",
    "import sys\n",
    "\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "text=open('../dataset/testing_text.txt').read()\n",
    "chars=['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
    "\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "maxlen=60\n",
    "step=1\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(1024, input_shape=(maxlen, len(chars)),return_sequences=True))\n",
    "model.add(layers.LSTM(1024, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "model.load_weights(\"../dataset/Sep-26-all-00-0.7280.hdf5\")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.0002)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def random_reviews():\n",
    "    start_index = np.random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('Coming up with several reviews for you...')\n",
    "\n",
    "    for temperature in [0.8]:\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 600 characters\n",
    "        for i in range(600):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coming up with several reviews for you...\n",
      "'This is a text file.  This file is just something to help ther"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jianghaitao1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e as well. The feta omelet is also very fresh.<EOR>\"\n",
      "\"<SOR>Grew up by noon beer employees replenished our drinks with my hubby there.  The steaks were excellent.  The spam used to make more than the kids to add in a potato.  I would definitely recommend this place to anyone.<EOR>\"\n",
      "\"<SOR>Had a nice strawberry ice cream with a side of ham and meatballs). Service was great. They play good food and they make the food arrived. There is minimal weekend spot for breakfast and lunch. Also they succeed you like people - Great place to get to the inflated food.<EOR>\"\n",
      "\"<SOR>Everything we had was delico the inflated food.<EOR>\"\n",
      "\"<SOR>Everything we had was delic\n"
     ]
    }
   ],
   "source": [
    "random_reviews()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Deep Learning models better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "def food_related(nouns):\n",
    "\n",
    "    food = wn.synset('food.n.01')\n",
    "    final_list = []\n",
    "    for word in nouns:\n",
    "        temp = word\n",
    "        word = word+'.n.01'\n",
    "        try:\n",
    "            if food.wup_similarity(wn.synset(word)) > 0.20 and temp != 'food':\n",
    "                final_list.append(temp)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "def user_custom(foods):\n",
    "    # enter foods as a string separated by commas. For example 'sushi, sashimi, maki'\n",
    "    start_index = np.random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('Coming up with two ideas for you...')\n",
    "\n",
    "    final = generated_text+''\n",
    "\n",
    "    for temperature in [0.8]:\n",
    "\n",
    "        # We generate 600 characters\n",
    "        for i in range(600):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "            final += next_char\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "    # print first review, then second via SOR/EOR\n",
    "    temp = personalized_clean_up(final, foods)\n",
    "    start = temp.find('SOR')\n",
    "    stop = findStrAfterStr(temp, 'EOR', 'SOR')\n",
    "    end_first = temp[start+4:stop]\n",
    "\n",
    "    new = temp[get_second_index(temp, 'SOR')+4:]\n",
    "    ending = new.find('EOR')\n",
    "    print(temp[start+4:stop])\n",
    "    print(\"\")\n",
    "    print(new[:ending])\n",
    "\n",
    "\n",
    "def personalized_clean_up(review, user_items):\n",
    "    # take generic review, and replace with user generated words\n",
    "    generic_nouns = review_to_nouns(review)\n",
    "    food_generic = food_related(generic_nouns)\n",
    "\n",
    "    user_picked_items = user_items.split(\",\")\n",
    "\n",
    "    final = []\n",
    "    for word in re.findall(r\"[\\w']+|[.,!?;]\", review):\n",
    "        if word in food_generic and len(user_picked_items) > 1:\n",
    "            word = np.random.choice(user_picked_items)\n",
    "            final.append(word)\n",
    "        else:\n",
    "            final.append(word)\n",
    "\n",
    "    new_review = \" \".join(final)\n",
    "    return re.sub(r'\\s+([?.!\",])', r'\\1', new_review)\n",
    "\n",
    "\n",
    "def review_to_nouns(review):\n",
    "    def is_noun(pos): return pos[:2] == 'NN'\n",
    "    token = nltk.word_tokenize(review)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(token) if is_noun(pos)]\n",
    "    return nouns\n",
    "\n",
    "\n",
    "def findStrAfterStr(myString, searchText, afterText):\n",
    "    after_index = myString.index(afterText)\n",
    "    return myString.find(searchText, after_index)\n",
    "\n",
    "\n",
    "def get_second_index(input_string, sub_string):\n",
    "    return input_string.index(sub_string, input_string.index(sub_string) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coming up with two ideas for you...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jianghaitao1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was so happy with the fresh oysters, and a great price. Get the most amazing  taco that I've found in the east  taco that serves fresh food  guac here! \n",
      "\n",
      "Excellent service. Loved the white  guac with a burrito of  guac  taco and  guac. burrito. My server Christina was so helpful. Will definitely return \n"
     ]
    }
   ],
   "source": [
    "user_custom('burrito, taco, guac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
