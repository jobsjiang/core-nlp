{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mp.weixin.qq.com/s?__biz=MzUyMzg0ODY0Ng==&mid=2247483839&idx=1&sn=e3a1e7753a283f9dcc8ecdf5354230eb&chksm=fa371f16cd409600b2de38a95376bd1f1d26919a67a804b30c53dfc96a066763ba2036315440&mpshare=1&scene=1&srcid=0330XnX8Q7x1BsPQCGHecsFv&sharer_sharetime=1585548755034&sharer_shareid=d88df2ec93a2e5e478eaa39ef5a82e2f&key=fa0f4cfef200c085620487de7d82882718619ab9893ba6f4b3100366149d2e86a8943803d07a5e6012ad4f5d6c64ccdf38ecc4d80ef93dc6f5a8742dff9388bba18d9b211bdd1a94a05f2fd7d897de7a&ascene=1&uin=MjA1MjAyODkxNg%3D%3D&devicetype=Windows+10&version=62080079&lang=zh_CN&exportkey=Ab9j2hoNaEcCHzzhs101Hxo%3D&pass_ticket=oQV%2B4QgocE%2B79igKS84ByQiBvNr7zSd0fGMluYqPBYLpNaLxouEPfg16iqZpY1vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# 数据路径\n",
    "DATA_PATH = '../dataset/poetry.txt'\n",
    "# 单行诗最大长度\n",
    "MAX_LEN = 64\n",
    "# 禁用的字符，拥有以下字符号的诗将被忽略\n",
    "DISALLOWED_WORDS = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']']\n",
    "# 一首诗（一行）对应一个列表的元素\n",
    "poetry = []\n",
    "# 按行读取数据 poetry.txt\n",
    "with open(DATA_PATH,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    # 遍历处理每条数据\n",
    "    for line in lines:\n",
    "        # 利用正则表达式拆分标题和内容\n",
    "        fields = re.split(r'[:]',line)\n",
    "        # 跳过异常数据\n",
    "        if len(fields) != 2:\n",
    "            continue\n",
    "        # 得到诗词内容，后面不需要标题\n",
    "        content = fields[1]\n",
    "        # 跳过内容过长的诗词\n",
    "        if len(content) > MAX_LEN - 2:\n",
    "            continue\n",
    "        # 跳过存在禁用符的诗词\n",
    "        if any(word in content for word in DISALLOWED_WORDS):\n",
    "            continue\n",
    "        poetry.append(content.replace('\\n','')) # 删除换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变，春逐鸟声开。初风飘带柳，晚雪间花梅。碧林青旧竹，绿沼翠新苔。芝田初雁去，绮树巧莺来。\n",
      "晚霞聊自怡，初晴弥可喜。日晃百花色，风动千林翠。池鱼跃不同，园鸟声还异。寄言博通者，知予物外志。\n",
      "夏律昨留灰，秋箭今移晷。峨嵋岫初出，洞庭波渐起。桂白发幽岩，菊黄开灞涘。运流方可叹，含毫属微理。\n",
      "寒惊蓟门叶，秋发小山枝。松阴背日转，竹影避风移。提壶菊花岸，高兴芙蓉池。欲知凉气早，巢空燕不窥。\n",
      "山亭秋色满，岩牖凉风度。疏兰尚染烟，残菊犹承露。古石衣新苔，新巢封古树。历览情无极，咫尺轮光暮。\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(poetry[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计一下词频，删除出现次数较低的词\n",
    "# 最小词频\n",
    "MIN_WORD_FREQUENCY = 8\n",
    "# 统计词频，利用Counter可以直接按单个字符进行统计词频\n",
    "counter = Counter()\n",
    "for line in poetry:\n",
    "    counter.update(line)\n",
    "# 过滤掉低词频的词\n",
    "tokens = [token for token,count in counter.items() if count >= MIN_WORD_FREQUENCY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒 -> 2628\n",
      "随 -> 1040\n",
      "穷 -> 487\n",
      "律 -> 119\n",
      "变 -> 288\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for token, count in counter.items():\n",
    "    if i >= 5:\n",
    "        break;\n",
    "    print(token, \"->\",count)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补上特殊词标记：填充字符标记、未知词标记、开始标记、结束标记\n",
    "tokens = [\"[PAD]\", \"[NONE]\", \"[START]\", \"[END]\"] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对生成的词进行编号\n",
    "# 映射: 词 -> 编号\n",
    "word_idx = {}\n",
    "# 映射: 编号 -> 词\n",
    "idx_word = {}\n",
    "for idx,word in enumerate(tokens):\n",
    "    word_idx[word] = idx\n",
    "    idx_word[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建Tokenizer\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    分词器\n",
    "    \"\"\"\n",
    "    def __init__(self,tokens):\n",
    "        # 词汇表大小\n",
    "        self.dict_size = len(tokens)\n",
    "        # 生成映射关系\n",
    "        self.token_id = {} # 映射：词 -> 编号\n",
    "        self.id_token = {} # 映射：编号 -> 词\n",
    "        for idx, word in enumerate(tokens):\n",
    "            self.token_id[word] = idx\n",
    "            self.id_token[idx] = word\n",
    "        # 各个特殊标记的编号id,方便其他地方使用\n",
    "        self.start_id = self.token_id[\"[START]\"]\n",
    "        self.end_id = self.token_id[\"[END]\"]\n",
    "        self.none_id = self.token_id[\"[NONE]\"]\n",
    "        self.pad_id = self.token_id[\"[PAD]\"]\n",
    "    def id_to_token(self,token_id):\n",
    "        \"\"\"\n",
    "        编号 -> 词\n",
    "        \"\"\"\n",
    "        return self.id_token.get(token_id)\n",
    "    def token_to_id(self,token):\n",
    "        \"\"\"\n",
    "        词 -> 编号\n",
    "        \"\"\"\n",
    "        return self.token_id.get(token,self.none_id)\n",
    "    def encode(self,tokens):\n",
    "        \"\"\"\n",
    "        词列表 -> [START]编号 + 编号列表 + [END]编号\n",
    "        \"\"\"\n",
    "        token_ids = [self.start_id,] # 起始标记\n",
    "        # 遍历，词转编号\n",
    "        for token in tokens:\n",
    "            token_ids.append(self.token_to_id(token))\n",
    "        token_ids.append(self.end_id)\n",
    "        return token_ids\n",
    "    def decode(self,token_ids):\n",
    "        \"\"\"\n",
    "        编号列表 -> 词列表(去掉起始，结束标记)\n",
    "        \"\"\"\n",
    "        # 起始，结束标记\n",
    "        flag_tokens = {\"[START]\", \"[END]\"}\n",
    "        tokens = []\n",
    "        for idx in token_ids:\n",
    "            token = self.id_to_token(idx)\n",
    "            # 跳过起始，结束标记\n",
    "            if token not in flag_tokens:\n",
    "                tokens.append(token)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化Tokenizer\n",
    "tokenizer = Tokenizer(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建PoetryDataSet\n",
    "class PoetryDataSet:\n",
    "    \"\"\"\n",
    "    生成数据集生成器\n",
    "    \"\"\"\n",
    "    def __init__(self,data,tokenizer,batch_size):\n",
    "        # 数据集\n",
    "        self.data = data\n",
    "        self.total_size = len(self.data)\n",
    "        # 分词器，用于词转编号\n",
    "        self.tokenizer = tokenizer\n",
    "        # 每批数据量\n",
    "        self.batch_size = batch_size\n",
    "        # 每个epoch迭代的次数\n",
    "        self.steps = int(math.floor(len(self.data) / self.batch_size))\n",
    "    def pad_line(self,line,length,padding=None):\n",
    "        \"\"\"\n",
    "        对齐单行数据\n",
    "        \"\"\"\n",
    "        if padding is None:\n",
    "            padding = self.tokenizer.pad_id\n",
    "        padding_length = length - len(line)\n",
    "        if padding_length > 0:\n",
    "            return line + [padding] * padding_length\n",
    "        else:\n",
    "            return line[:length]\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    def __iter__(self):\n",
    "        # 打乱数据\n",
    "        np.random.shuffle(self.data)\n",
    "        # 迭代一个epoch,每次yield一个batch\n",
    "        for start in range(0,self.total_size,self.batch_size):\n",
    "            end = min(start + self.batch_size,self.total_size)\n",
    "            data = self.data[start:end]\n",
    "            max_length = max(map(len,data))\n",
    "            batch_data = []\n",
    "            for str_line in data:\n",
    "                # 对每一行诗词进行编码，并补齐padding\n",
    "                encode_line = self.tokenizer.encode(str_line)\n",
    "                pad_encode_line = self.pad_line(encode_line,max_length+2) # 加2是因为tokenizer.encode会添加START和END\n",
    "                batch_data.append(pad_encode_line)\n",
    "            batch_data = np.array(batch_data)\n",
    "            # yield 特征，标签\n",
    "            yield batch_data[:,:-1],batch_data[:,1:]\n",
    "    def generator(self):\n",
    "        while True:\n",
    "            yield from self.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "dataset = PoetryDataSet(poetry,tokenizer,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "model = tf.keras.Sequential([\n",
    "    # 词嵌入层\n",
    "    tf.keras.layers.Embedding(input_dim=tokenizer.dict_size,output_dim=150),\n",
    "    # 第一层LSTM层\n",
    "    tf.keras.layers.LSTM(150,dropout=0.5,return_sequences=True),\n",
    "    # 第二层LSTM层\n",
    "    tf.keras.layers.LSTM(150,dropout=0.5,return_sequences=True),\n",
    "    # 利用TimeDistributed对每个时间步的输出都做Dense操作（softmax激活）\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tokenizer.dict_size,activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 150)         515250    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 150)         180600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 150)         180600    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 3435)        518685    \n",
      "=================================================================\n",
      "Total params: 1,395,135\n",
      "Trainable params: 1,395,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型编译\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1535/1535 [==============================] - 161s 105ms/step - loss: 4.8718\n",
      "Epoch 2/10\n",
      "1535/1535 [==============================] - 169s 110ms/step - loss: 4.3652\n",
      "Epoch 3/10\n",
      "1535/1535 [==============================] - 172s 112ms/step - loss: 4.2012\n",
      "Epoch 4/10\n",
      "1535/1535 [==============================] - 169s 110ms/step - loss: 4.0196\n",
      "Epoch 5/10\n",
      "1535/1535 [==============================] - 169s 110ms/step - loss: 3.9258\n",
      "Epoch 6/10\n",
      "1535/1535 [==============================] - 173s 113ms/step - loss: 3.8601\n",
      "Epoch 7/10\n",
      "1535/1535 [==============================] - 172s 112ms/step - loss: 3.8158\n",
      "Epoch 8/10\n",
      "1535/1535 [==============================] - 170s 111ms/step - loss: 3.7776\n",
      "Epoch 9/10\n",
      "1535/1535 [==============================] - 178s 116ms/step - loss: 3.7390\n",
      "Epoch 10/10\n",
      "1535/1535 [==============================] - 171s 112ms/step - loss: 3.7092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1acc91e7898>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(\n",
    "    dataset.generator(),\n",
    "    steps_per_epoch=dataset.steps,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "# 需要先将词转为编号\n",
    "token_ids = [tokenizer.token_to_id(word) for word in [\"月\", \"光\", \"静\", \"谧\"]]\n",
    "# 进行预测\n",
    "result = model.predict([token_ids,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4.8102291e-05 1.0659970e-02 1.9756358e-06 ... 6.4293868e-06\n",
      "   9.1382490e-06 2.0419389e-05]\n",
      "  [1.0790689e-06 1.4589122e-02 1.2457634e-08 ... 4.3525708e-07\n",
      "   5.6293044e-07 6.2695214e-08]\n",
      "  [6.8445984e-06 8.2051912e-03 8.4110994e-09 ... 5.7726442e-07\n",
      "   3.0468429e-06 4.0382113e-07]\n",
      "  [1.1436045e-04 1.1967261e-02 6.7134884e-08 ... 1.8703339e-08\n",
      "   1.1075260e-07 1.0855107e-06]]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要词的多样化，因此可以按预测结果的概率分布进行抽样\n",
    "def predict(model,token_ids):\n",
    "    \"\"\"\n",
    "    在概率值为前100的词中选取一个词（按概率分布的方式）\n",
    "    return:一个词的编号（不包含[PAD][NONE][START]）\n",
    "    \"\"\"\n",
    "    # 预测各个词的概率分布\n",
    "    # -1 表示只要对最新的词的预测\n",
    "    # 3 表示不要前面几个标记符\n",
    "    _probas = model.predict([token_ids,])[0,-1,3:]\n",
    "    # 按概率升序，取前100\n",
    "    p_args = _probas.argsort()[-100:][::-1] # 此时拿到的是索引\n",
    "    p = _probas[p_args] # 根据索引找到具体的概率值\n",
    "    p = p / sum(p) # 归一\n",
    "    # 按概率抽取一个\n",
    "    target_index = np.random.choice(len(p),p=p)\n",
    "    # 前面预测时删除了前几个标记符，因此编号要补上3位，才是实际在tokenizer词典中的编号\n",
    "    return p_args[target_index] + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0408 12:09:38.875421 16608 def_function.py:597] 5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001ACC79700D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "W0408 12:09:39.472723 16608 def_function.py:597] 6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001ACC79700D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "W0408 12:09:39.973383 16608 def_function.py:597] 7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001ACC79700D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "W0408 12:09:40.419191 16608 def_function.py:597] 8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001ACC79700D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "W0408 12:09:40.903895 16608 def_function.py:597] 9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001ACC79700D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清风明月接，雨水夜山还。\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\"清风明月\")[:-1]\n",
    "while len(token_ids) < 13:\n",
    "    # 预测词的编号\n",
    "    target = predict(model,token_ids)\n",
    "    # 保存结果\n",
    "    token_ids.append(target)\n",
    "    # 到达END\n",
    "    if target == tokenizer.end_id:\n",
    "        break\n",
    "print(''.join(tokenizer.decode(token_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
