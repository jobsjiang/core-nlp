{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mp.weixin.qq.com/s?__biz=MzUyMzg0ODY0Ng==&mid=2247483839&idx=1&sn=e3a1e7753a283f9dcc8ecdf5354230eb&chksm=fa371f16cd409600b2de38a95376bd1f1d26919a67a804b30c53dfc96a066763ba2036315440&mpshare=1&scene=1&srcid=0330XnX8Q7x1BsPQCGHecsFv&sharer_sharetime=1585548755034&sharer_shareid=d88df2ec93a2e5e478eaa39ef5a82e2f&key=fa0f4cfef200c085620487de7d82882718619ab9893ba6f4b3100366149d2e86a8943803d07a5e6012ad4f5d6c64ccdf38ecc4d80ef93dc6f5a8742dff9388bba18d9b211bdd1a94a05f2fd7d897de7a&ascene=1&uin=MjA1MjAyODkxNg%3D%3D&devicetype=Windows+10&version=62080079&lang=zh_CN&exportkey=Ab9j2hoNaEcCHzzhs101Hxo%3D&pass_ticket=oQV%2B4QgocE%2B79igKS84ByQiBvNr7zSd0fGMluYqPBYLpNaLxouEPfg16iqZpY1vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# 数据路径\n",
    "DATA_PATH = '../dataset/poetry.txt'\n",
    "# 单行诗最大长度\n",
    "MAX_LEN = 64\n",
    "# 禁用的字符，拥有以下字符号的诗将被忽略\n",
    "DISALLOWED_WORDS = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']']\n",
    "# 一首诗（一行）对应一个列表的元素\n",
    "poetry = []\n",
    "# 按行读取数据 poetry.txt\n",
    "with open(DATA_PATH,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    # 遍历处理每条数据\n",
    "    for line in lines:\n",
    "        # 利用正则表达式拆分标题和内容\n",
    "        fields = re.split(r'[:]',line)\n",
    "        # 跳过异常数据\n",
    "        if len(fields) != 2:\n",
    "            continue\n",
    "        # 得到诗词内容，后面不需要标题\n",
    "        content = fields[1]\n",
    "        # 跳过内容过长的诗词\n",
    "        if len(content) > MAX_LEN - 2:\n",
    "            continue\n",
    "        # 跳过存在禁用符的诗词\n",
    "        if any(word in content for word in DISALLOWED_WORDS):\n",
    "            continue\n",
    "        poetry.append(content.replace('\\n','')) # 删除换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变，春逐鸟声开。初风飘带柳，晚雪间花梅。碧林青旧竹，绿沼翠新苔。芝田初雁去，绮树巧莺来。\n",
      "晚霞聊自怡，初晴弥可喜。日晃百花色，风动千林翠。池鱼跃不同，园鸟声还异。寄言博通者，知予物外志。\n",
      "夏律昨留灰，秋箭今移晷。峨嵋岫初出，洞庭波渐起。桂白发幽岩，菊黄开灞涘。运流方可叹，含毫属微理。\n",
      "寒惊蓟门叶，秋发小山枝。松阴背日转，竹影避风移。提壶菊花岸，高兴芙蓉池。欲知凉气早，巢空燕不窥。\n",
      "山亭秋色满，岩牖凉风度。疏兰尚染烟，残菊犹承露。古石衣新苔，新巢封古树。历览情无极，咫尺轮光暮。\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(poetry[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计一下词频，删除出现次数较低的词\n",
    "# 最小词频\n",
    "MIN_WORD_FREQUENCY = 8\n",
    "# 统计词频，利用Counter可以直接按单个字符进行统计词频\n",
    "counter = Counter()\n",
    "for line in poetry:\n",
    "    counter.update(line)\n",
    "# 过滤掉低词频的词\n",
    "tokens = [token for token,count in counter.items() if count >= MIN_WORD_FREQUENCY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒 -> 2628\n",
      "随 -> 1040\n",
      "穷 -> 487\n",
      "律 -> 119\n",
      "变 -> 288\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for token, count in counter.items():\n",
    "    if i >= 5:\n",
    "        break;\n",
    "    print(token, \"->\",count)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补上特殊词标记：填充字符标记、未知词标记、开始标记、结束标记\n",
    "tokens = [\"[PAD]\", \"[NONE]\", \"[START]\", \"[END]\"] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对生成的词进行编号\n",
    "# 映射: 词 -> 编号\n",
    "word_idx = {}\n",
    "# 映射: 编号 -> 词\n",
    "idx_word = {}\n",
    "for idx,word in enumerate(tokens):\n",
    "    word_idx[word] = idx\n",
    "    idx_word[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建Tokenizer\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    分词器\n",
    "    \"\"\"\n",
    "    def __init__(self,tokens):\n",
    "        # 词汇表大小\n",
    "        self.dict_size = len(tokens)\n",
    "        # 生成映射关系\n",
    "        self.token_id = {} # 映射：词 -> 编号\n",
    "        self.id_token = {} # 映射：编号 -> 词\n",
    "        # 各个特殊标记的编号id,方便其他地方使用\n",
    "        self.start_id = self.token_id['[START]']\n",
    "        self.end_id = self.token_id[\"[END]\"]\n",
    "        self.none_id = self.token_id[\"[NONE]\"]\n",
    "        self.pad_id = self.token_id[\"[PAD]\"]\n",
    "    def id_to_token(self,token_id):\n",
    "        \"\"\"\n",
    "        编号 -> 词\n",
    "        \"\"\"\n",
    "        return self.id_token.get(token_id)\n",
    "    def token_to_id(self,token):\n",
    "        \"\"\"\n",
    "        词 -> 编号\n",
    "        \"\"\"\n",
    "        return self.token_id.get(token,self.none_id)\n",
    "    def encode(self,tokens):\n",
    "        \"\"\"\n",
    "        词列表 -> [START]编号 + 编号列表 + [END]编号\n",
    "        \"\"\"\n",
    "        token_ids = [self.start_id,] # 起始标记\n",
    "        # 遍历，词转编号\n",
    "        for token in tokens:\n",
    "            token_ids.append(self.token_to_id(token))\n",
    "        token_ids.append(self.end_id)\n",
    "        return token_ids\n",
    "    def decode(self,token_ids):\n",
    "        \"\"\"\n",
    "        编号列表 -> 词列表(去掉起始，结束标记)\n",
    "        \"\"\"\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
